{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIUDFISI1qvkTdjGKJsh65",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Michaelkats5/DemandSync/blob/main/Monte_Carlo_Simulation_for_College_Football_Team_Rankings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coe3JJDY9tgn"
      },
      "outputs": [],
      "source": [
        "# ==================== 1. IMPORTS ====================\n",
        "import os, glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from io import StringIO\n",
        "\n",
        "# ==================== 2. BASE ARRAY (MANUAL DATA) ====================\n",
        "# Replace this block with your manually curated array\n",
        "teams_data = [\n",
        "    {\"team\": \"Texas\", \"conf\": \"SEC\", \"FPI\": 24.5, \"TalentRank\": 4,\n",
        "     \"TalentPoints\": 973.54, \"ReturningStartersPct\": 55.0, \"QB_Snaps\": 260},\n",
        "    {\"team\": \"Alabama\", \"conf\": \"SEC\", \"FPI\": 20.7, \"TalentRank\": 2,\n",
        "     \"TalentPoints\": 993.55, \"ReturningStartersPct\": 63.6, \"QB_Snaps\": 60},\n",
        "    {\"team\": \"Ohio State\", \"conf\": \"Big Ten\", \"FPI\": 20.5, \"TalentRank\": 3,\n",
        "     \"TalentPoints\": 973.69, \"ReturningStartersPct\": 50.0, \"QB_Snaps\": 180},\n",
        "    {\"team\": \"Georgia\", \"conf\": \"SEC\", \"FPI\": 19.8, \"TalentRank\": 1,\n",
        "     \"TalentPoints\": 999.9, \"ReturningStartersPct\": 58.0, \"QB_Snaps\": 40},\n",
        "    {\"team\": \"Penn State\", \"conf\": \"Big Ten\", \"FPI\": 18.2, \"TalentRank\": 6,\n",
        "     \"TalentPoints\": 950.1, \"ReturningStartersPct\": 62.0, \"QB_Snaps\": 150},\n",
        "    {\"team\": \"Clemson\", \"conf\": \"ACC\", \"FPI\": 17.0, \"TalentRank\": 7,\n",
        "     \"TalentPoints\": 940.0, \"ReturningStartersPct\": 60.0, \"QB_Snaps\": 220},\n",
        "    {\"team\": \"Notre Dame\", \"conf\": \"Ind\", \"FPI\": 16.5, \"TalentRank\": 8,\n",
        "     \"TalentPoints\": 930.0, \"ReturningStartersPct\": 61.0, \"QB_Snaps\": 75},\n",
        "    {\"team\": \"Oregon\", \"conf\": \"Pac-12\", \"FPI\": 15.8, \"TalentRank\": 5,\n",
        "     \"TalentPoints\": 960.0, \"ReturningStartersPct\": 59.0, \"QB_Snaps\": 100},\n",
        "    {\"team\": \"LSU\", \"conf\": \"SEC\", \"FPI\": 15.2, \"TalentRank\": 9,\n",
        "     \"TalentPoints\": 925.0, \"ReturningStartersPct\": 57.0, \"QB_Snaps\": 210},\n",
        "    {\"team\": \"Miami\", \"conf\": \"ACC\", \"FPI\": 14.9, \"TalentRank\": 10,\n",
        "     \"TalentPoints\": 915.0, \"ReturningStartersPct\": 56.0, \"QB_Snaps\": 190},\n",
        "]\n",
        "df_array = pd.DataFrame(teams_data)\n",
        "df_array[\"team\"] = df_array[\"team\"].astype(str).str.strip()\n",
        "print(\"Base array loaded:\")\n",
        "print(df_array.head())\n",
        "\n",
        "# ==================== 3. LOAD TXT FILES ====================\n",
        "SEARCH_DIRS = [\".\", \"/content\", \"/content/drive/MyDrive\"]\n",
        "PATTERN = \"*_organized_with_coach*.txt\"\n",
        "\n",
        "paths = []\n",
        "for d in SEARCH_DIRS:\n",
        "    paths += glob.glob(os.path.join(d, PATTERN))\n",
        "paths = sorted(set(paths))\n",
        "\n",
        "print(f\"\\nFound {len(paths)} text files:\")\n",
        "for p in paths: print(\" -\", p)\n",
        "\n",
        "def parse_team_file(path: str) -> pd.DataFrame:\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "        text = f.read().replace(\"\\ufeff\", \"\")\n",
        "    header_key = \"team,conference,coach\"\n",
        "    start = text.lower().find(header_key)\n",
        "    if start == -1:\n",
        "        raise ValueError(f\"No CSV header in {path}\")\n",
        "    csv_block = text[start:]\n",
        "    df = pd.read_csv(StringIO(csv_block))\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "    df[\"team\"] = df[\"team\"].astype(str).str.strip()\n",
        "    rename_map = {\n",
        "        \"conference\": \"conf\",\n",
        "        \"ovr_rating\": \"OVR\",\n",
        "        \"off_rating\": \"OFF\",\n",
        "        \"def_rating\": \"DEF\",\n",
        "        \"qb_avg\": \"QB_AVG\",\n",
        "        \"career_win_pct\": \"coach_career_win_pct\",\n",
        "    }\n",
        "    df = df.rename(columns=rename_map)\n",
        "    keep = [c for c in [\"team\",\"conf\",\"coach\",\"coach_career_win_pct\",\"OVR\",\"OFF\",\"DEF\",\"QB_AVG\"] if c in df.columns]\n",
        "    return df[keep]\n",
        "\n",
        "dfs = []\n",
        "for p in paths:\n",
        "    try:\n",
        "        dfs.append(parse_team_file(p))\n",
        "        print(f\"[OK ] {os.path.basename(p)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[BAD] {os.path.basename(p)} -> {e}\")\n",
        "\n",
        "teams_extra = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
        "print(\"\\nExtra data loaded:\")\n",
        "print(teams_extra.head())\n",
        "\n",
        "# ==================== 4. MERGE MASTER DATA ====================\n",
        "df = df_array.merge(teams_extra, on=\"team\", how=\"left\")\n",
        "print(\"\\nMerged dataset preview:\")\n",
        "print(df.head())\n",
        "\n",
        "# ==================== 5. FAIR RATING CALCULATION ====================\n",
        "def robust_z(s: pd.Series):\n",
        "    med = s.median()\n",
        "    iqr = s.quantile(0.75) - s.quantile(0.25)\n",
        "    if iqr == 0:\n",
        "        std = s.std(ddof=0) + 1e-9\n",
        "        return (s - med) / std\n",
        "    return (s - med) / (iqr + 1e-9)\n",
        "\n",
        "numeric = [\"FPI\",\"OVR\",\"OFF\",\"DEF\",\"QB_AVG\",\"TalentPoints\",\"ReturningStartersPct\",\"QB_Snaps\"]\n",
        "for c in numeric:\n",
        "    if c in df.columns:\n",
        "        df[c+\"_rz\"] = robust_z(df[c]).clip(-1.5, 1.5)\n",
        "\n",
        "weights = {\n",
        "    \"FPI_rz\": 0.25, \"DEF_rz\": 0.18, \"OFF_rz\": 0.14, \"OVR_rz\": 0.10,\n",
        "    \"QB_AVG_rz\": 0.12, \"TalentPoints_rz\": 0.08, \"ReturningStartersPct_rz\": 0.07, \"QB_Snaps_rz\": 0.06\n",
        "}\n",
        "weights = {k:v for k,v in weights.items() if k in df.columns}\n",
        "total = sum(weights.values())\n",
        "weights = {k: v/total for k,v in weights.items()}\n",
        "\n",
        "df[\"feature_score\"] = sum(df[k]*w for k,w in weights.items())\n",
        "df[\"rating_fair_raw\"] = 0.7 * df[\"feature_score\"]\n",
        "mu, sd = df[\"rating_fair_raw\"].mean(), df[\"rating_fair_raw\"].std(ddof=0)+1e-9\n",
        "df[\"rating\"] = (df[\"rating_fair_raw\"] - mu)/sd\n",
        "\n",
        "print(\"\\nTop teams by fair rating:\")\n",
        "print(df[[\"team\",\"rating\"]].sort_values(\"rating\",ascending=False))\n",
        "\n",
        "# ==================== 6. WIN PROBABILITY FUNCTION ====================\n",
        "SCALE = 6.0\n",
        "def win_prob(r1, r2, scale=SCALE):\n",
        "    return 1 / (1 + np.exp(-(r1-r2)/scale))\n",
        "\n",
        "print(\"\\nWin probability Texas vs Georgia:\", win_prob(\n",
        "    float(df.loc[df.team==\"Texas\",\"rating\"]),\n",
        "    float(df.loc[df.team==\"Georgia\",\"rating\"])\n",
        "))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, os\n",
        "\n",
        "# Look for all your *_organized_with_coach*.txt files\n",
        "files = glob.glob(\"*_organized_with_coach*.txt\")\n",
        "print(f\"Found {len(files)} files:\")\n",
        "for f in files:\n",
        "    print(\" -\", f)\n",
        "\n",
        "# Show the first 30 lines of each file\n",
        "for f in files:\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(f\"FILE: {f}\")\n",
        "    print(\"=\"*40)\n",
        "    try:\n",
        "        with open(f, \"r\", encoding=\"utf-8\", errors=\"replace\") as fh:\n",
        "            for i, line in enumerate(fh):\n",
        "                print(line.strip())\n",
        "                if i >= 29:  # stop after 30 lines\n",
        "                    break\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Could not read {f}: {e}\")\n"
      ],
      "metadata": {
        "id": "C-PXimqR-sQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# NCAA Monte Carlo (3,000 trials) using your textfiles + arrays\n",
        "# =========================\n",
        "\n",
        "# --- Imports (the ones you asked about + a couple standard ones) ---\n",
        "import os, glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from io import StringIO\n",
        "\n",
        "# -------------------------\n",
        "# 1) Load ALL tables from disk (.txt/.csv) with flexible parsing\n",
        "# -------------------------\n",
        "def read_any_table(path):\n",
        "    \"\"\"\n",
        "    Try to read a table with unknown delimiter:\n",
        "    - pandas automatic sep=None (engine='python')\n",
        "    - fallback to whitespace\n",
        "    - fallback to CSV with comma\n",
        "    Returns DataFrame, or None if unreadable.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return pd.read_csv(path, sep=None, engine=\"python\")\n",
        "    except Exception:\n",
        "        try:\n",
        "            return pd.read_csv(path, delim_whitespace=True, engine=\"python\")\n",
        "        except Exception:\n",
        "            try:\n",
        "                return pd.read_csv(path)\n",
        "            except Exception:\n",
        "                return None\n",
        "\n",
        "def load_all_local_tables(search_dirs=(\".\", \"./data\", \"/content\")):\n",
        "    \"\"\"\n",
        "    Search common Colab/local folders for .txt and .csv files\n",
        "    and return a concatenated DataFrame (row-bind). Missing columns are kept.\n",
        "    \"\"\"\n",
        "    paths = []\n",
        "    for d in search_dirs:\n",
        "        if os.path.isdir(d):\n",
        "            paths.extend(glob.glob(os.path.join(d, \"*.txt\")))\n",
        "            paths.extend(glob.glob(os.path.join(d, \"*.csv\")))\n",
        "\n",
        "    frames = []\n",
        "    for p in paths:\n",
        "        df = read_any_table(p)\n",
        "        if df is not None and len(df) > 0:\n",
        "            df[\"__source_file\"] = os.path.basename(p)\n",
        "            frames.append(df)\n",
        "\n",
        "    if frames:\n",
        "        return pd.concat(frames, ignore_index=True, sort=False)\n",
        "    else:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "files_df = load_all_local_tables()\n",
        "\n",
        "# -------------------------\n",
        "# 2) Pull in arrays you added (if they exist) e.g., teams_data = [{...}, ...]\n",
        "# -------------------------\n",
        "arrays_df = pd.DataFrame()\n",
        "try:\n",
        "    # If you defined teams_data earlier in your notebook, we'll use it.\n",
        "    if isinstance(teams_data, (list, tuple)) and len(teams_data) > 0:\n",
        "        arrays_df = pd.DataFrame(teams_data)\n",
        "except NameError:\n",
        "    pass  # no arrays provided yet; that's fine\n",
        "\n",
        "# If you created other arrays (e.g., ratings_data, coaches_data), append them here:\n",
        "for var_name in [\"ratings_data\", \"coaches_data\", \"roster_data\"]:\n",
        "    try:\n",
        "        _val = globals().get(var_name, None)\n",
        "        if isinstance(_val, (list, tuple)) and len(_val) > 0:\n",
        "            arrays_df = pd.concat([arrays_df, pd.DataFrame(_val)], ignore_index=True, sort=False)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# -------------------------\n",
        "# 3) Combine file tables + arrays, standardize column names\n",
        "# -------------------------\n",
        "raw = pd.concat([files_df, arrays_df], ignore_index=True, sort=False)\n",
        "\n",
        "# Canonical names we’ll try to map to:\n",
        "# team, conf, FPI, TalentPoints, ReturningStartersPct, QB_Snaps, coach_career_win_pct, OVR, OFF, DEF, QB_AVG\n",
        "rename_map_candidates = {\n",
        "    \"team\": [\"team\", \"Team\", \"school\", \"School\", \"program\", \"Program\"],\n",
        "    \"conf\": [\"conf\", \"conference\", \"Conference\", \"Conf\"],\n",
        "    \"FPI\": [\"FPI\", \"fpi\", \"espn_fpi\", \"espnFPI\"],\n",
        "    \"TalentPoints\": [\"TalentPoints\", \"talent_points\", \"Talent\", \"Talent_Points\", \"BlueChipPoints\", \"blue_chip_points\"],\n",
        "    \"ReturningStartersPct\": [\"ReturningStartersPct\", \"ReturningStarters%\", \"ReturningPct\", \"returning_pct\", \"Returning_Starters\"],\n",
        "    \"QB_Snaps\": [\"QB_Snaps\", \"QB Snaps\", \"qb_snaps\", \"qb_total_snaps\"],\n",
        "    \"coach_career_win_pct\": [\"coach_career_win_pct\", \"CoachWinPct\", \"coach_win_pct\", \"coachCareerWinPct\", \"Coach_Career_Win_Pct\"],\n",
        "    \"OVR\": [\"OVR\", \"Overall\", \"overall_rating\", \"overall\"],\n",
        "    \"OFF\": [\"OFF\", \"Offense\", \"off_rating\", \"OFF_Rating\"],\n",
        "    \"DEF\": [\"DEF\", \"Defense\", \"def_rating\", \"DEF_Rating\"],\n",
        "    \"QB_AVG\": [\"QB_AVG\", \"QB_Avg\", \"QB_Rating\", \"qb_overall\", \"QB\"]\n",
        "}\n",
        "\n",
        "def smart_rename_columns(df):\n",
        "    colmap = {}\n",
        "    lower_cols = {c.lower(): c for c in df.columns}\n",
        "    for canonical, options in rename_map_candidates.items():\n",
        "        for opt in options:\n",
        "            if opt in df.columns:\n",
        "                colmap[opt] = canonical\n",
        "                break\n",
        "            # also try case-insensitive\n",
        "            opt_low = opt.lower()\n",
        "            if opt_low in lower_cols:\n",
        "                colmap[lower_cols[opt_low]] = canonical\n",
        "                break\n",
        "    return df.rename(columns=colmap)\n",
        "\n",
        "if raw.empty:\n",
        "    raise ValueError(\"No data found. Ensure your .txt/.csv files are in the working directory or define arrays like `teams_data`.\")\n",
        "\n",
        "raw = smart_rename_columns(raw)\n",
        "\n",
        "# Keep only relevant columns (others are fine to keep; we’ll ignore them)\n",
        "wanted_cols = [\"team\",\"conf\",\"FPI\",\"TalentPoints\",\"ReturningStartersPct\",\"QB_Snaps\",\"coach_career_win_pct\",\"OVR\",\"OFF\",\"DEF\",\"QB_AVG\",\"__source_file\"]\n",
        "present_cols = [c for c in wanted_cols if c in raw.columns]\n",
        "data = raw[present_cols].copy()\n",
        "\n",
        "# Deduplicate by team (keep the last occurrence which is usually the freshest you loaded)\n",
        "if \"team\" in data.columns:\n",
        "    data = data.dropna(subset=[\"team\"]).drop_duplicates(subset=[\"team\"], keep=\"last\")\n",
        "else:\n",
        "    raise ValueError(\"A 'team' column is required in your files/arrays. Please include a team name for each row.\")\n",
        "\n",
        "# Coerce numerics where appropriate\n",
        "for num_col in [\"FPI\",\"TalentPoints\",\"ReturningStartersPct\",\"QB_Snaps\",\"coach_career_win_pct\",\"OVR\",\"OFF\",\"DEF\",\"QB_AVG\"]:\n",
        "    if num_col in data.columns:\n",
        "        data[num_col] = pd.to_numeric(data[num_col], errors=\"coerce\")\n",
        "\n",
        "# -------------------------\n",
        "# 4) Normalize features to avoid one team dominating\n",
        "#    (min-max to [0,1] per feature that exists)\n",
        "# -------------------------\n",
        "feature_weights = {\n",
        "    \"FPI\": 0.20,\n",
        "    \"TalentPoints\": 0.15,\n",
        "    \"ReturningStartersPct\": 0.10,\n",
        "    \"QB_Snaps\": 0.10,\n",
        "    \"coach_career_win_pct\": 0.10,\n",
        "    \"OVR\": 0.15,\n",
        "    \"OFF\": 0.10,\n",
        "    \"DEF\": 0.07,\n",
        "    \"QB_AVG\": 0.03\n",
        "}\n",
        "# Keep only weights for columns that actually exist in your data\n",
        "feature_weights = {k:v for k,v in feature_weights.items() if k in data.columns}\n",
        "\n",
        "# Re-normalize weights so they sum to 1.0 even if some columns are missing\n",
        "w_sum = sum(feature_weights.values())\n",
        "feature_weights = {k: v / w_sum for k,v in feature_weights.items()}\n",
        "\n",
        "def minmax(series):\n",
        "    s = series.astype(float)\n",
        "    lo, hi = s.min(), s.max()\n",
        "    if pd.isna(lo) or pd.isna(hi) or lo == hi:\n",
        "        # If constant or NaN, return 0.5 everywhere to avoid bias\n",
        "        return pd.Series([0.5]*len(s), index=s.index)\n",
        "    return (s - lo) / (hi - lo)\n",
        "\n",
        "norm = pd.DataFrame(index=data.index)\n",
        "for feat in feature_weights.keys():\n",
        "    norm[f\"norm_{feat}\"] = minmax(data[feat])\n",
        "\n",
        "# -------------------------\n",
        "# 5) Strength score (weighted sum of normalized features)\n",
        "# -------------------------\n",
        "def compute_strength(row):\n",
        "    s = 0.0\n",
        "    for feat, w in feature_weights.items():\n",
        "        s += w * row[f\"norm_{feat}\"]\n",
        "    return s\n",
        "\n",
        "data[\"strength\"] = norm.apply(compute_strength, axis=1)\n",
        "\n",
        "# (Optional) small jitter to break ties without changing overall fairness\n",
        "np.random.seed(42)\n",
        "data[\"strength\"] = data[\"strength\"] + np.random.normal(0, 1e-6, size=len(data))\n",
        "\n",
        "# -------------------------\n",
        "# 6) Choose teams: use all, or cap at 10 by strength\n",
        "# -------------------------\n",
        "USE_TOP_10 = True  # set to False to use every team you loaded\n",
        "if USE_TOP_10:\n",
        "    sim_df = data.sort_values(\"strength\", ascending=False).head(10).reset_index(drop=True)\n",
        "else:\n",
        "    sim_df = data.reset_index(drop=True)\n",
        "\n",
        "# -------------------------\n",
        "# 7) Monte Carlo simulation (3,000 trials), round-robin\n",
        "# -------------------------\n",
        "def simulate_game(str_a, str_b):\n",
        "    # Win prob via Bradley-Terry style model\n",
        "    p_a = str_a / (str_a + str_b)\n",
        "    return np.random.rand() < p_a\n",
        "\n",
        "def monte_carlo_round_robin(df, trials=3000, seed=123):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    teams = df[\"team\"].tolist()\n",
        "    strengths = df[\"strength\"].to_numpy()\n",
        "    idx = np.arange(len(teams))\n",
        "\n",
        "    wins = np.zeros(len(teams), dtype=np.int64)\n",
        "\n",
        "    for _ in range(trials):\n",
        "        # every unordered pair once per trial\n",
        "        for i in range(len(teams)):\n",
        "            for j in range(i+1, len(teams)):\n",
        "                p_i = strengths[i] / (strengths[i] + strengths[j])\n",
        "                # sample winner\n",
        "                if rng.random() < p_i:\n",
        "                    wins[i] += 1\n",
        "                else:\n",
        "                    wins[j] += 1\n",
        "\n",
        "    total_games = (len(teams) * (len(teams) - 1) // 2) * trials\n",
        "    # convert to share of all games won; this is not \"title odds\" but a robust power ranking proxy\n",
        "    win_share = wins / total_games\n",
        "    out = pd.DataFrame({\"Team\": teams, \"Win_Share\": win_share})\n",
        "    # also include the normalized strength for transparency\n",
        "    out[\"Strength\"] = df[\"strength\"].to_numpy()\n",
        "    return out.sort_values(\"Win_Share\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "results = monte_carlo_round_robin(sim_df, trials=3000, seed=2025)\n",
        "\n",
        "# -------------------------\n",
        "# 8) Display results\n",
        "# -------------------------\n",
        "pd.set_option(\"display.precision\", 4)\n",
        "print(\"\\n=== Monte Carlo Results (3,000 trials) ===\")\n",
        "print(results)\n",
        "\n",
        "print(\"\\n(Info) Teams included:\")\n",
        "print(sim_df[[\"team\",\"conf\"]].fillna(\"N/A\") if \"conf\" in sim_df.columns else sim_df[[\"team\"]])\n",
        "\n",
        "print(\"\\n(Info) Features used & weights after availability check:\")\n",
        "print(feature_weights)\n",
        "\n",
        "# Optional: write results to CSV for your records\n",
        "results.to_csv(\"monte_carlo_results.csv\", index=False)\n",
        "print(\"\\nSaved: monte_carlo_results.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "I8FsvreX_qzf",
        "outputId": "d861048b-e880-44ee-889b-021b5b898b57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3604557381.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(path, delim_whitespace=True, engine=\"python\")\n",
            "/tmp/ipython-input-3604557381.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(path, delim_whitespace=True, engine=\"python\")\n",
            "/tmp/ipython-input-3604557381.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(path, delim_whitespace=True, engine=\"python\")\n",
            "/tmp/ipython-input-3604557381.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(path, delim_whitespace=True, engine=\"python\")\n",
            "/tmp/ipython-input-3604557381.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(path, delim_whitespace=True, engine=\"python\")\n",
            "/tmp/ipython-input-3604557381.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(path, delim_whitespace=True, engine=\"python\")\n",
            "/tmp/ipython-input-3604557381.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(path, delim_whitespace=True, engine=\"python\")\n",
            "/tmp/ipython-input-3604557381.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(path, delim_whitespace=True, engine=\"python\")\n",
            "/tmp/ipython-input-3604557381.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(path, delim_whitespace=True, engine=\"python\")\n",
            "/tmp/ipython-input-3604557381.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(path, delim_whitespace=True, engine=\"python\")\n",
            "/tmp/ipython-input-3604557381.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(path, delim_whitespace=True, engine=\"python\")\n",
            "/tmp/ipython-input-3604557381.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(path, delim_whitespace=True, engine=\"python\")\n",
            "/tmp/ipython-input-3604557381.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(path, delim_whitespace=True, engine=\"python\")\n",
            "/tmp/ipython-input-3604557381.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(path, delim_whitespace=True, engine=\"python\")\n",
            "/tmp/ipython-input-3604557381.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(path, delim_whitespace=True, engine=\"python\")\n",
            "/tmp/ipython-input-3604557381.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  return pd.read_csv(path, delim_whitespace=True, engine=\"python\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Monte Carlo Results (3,000 trials) ===\n",
            "         Team  Win_Share  Strength\n",
            "0       Texas     0.1340    0.8004\n",
            "1     Alabama     0.1239    0.6704\n",
            "2     Georgia     0.1142    0.5653\n",
            "3  Ohio State     0.1104    0.5164\n",
            "4  Penn State     0.1075    0.4891\n",
            "5     Clemson     0.1017    0.4423\n",
            "6      Oregon     0.0899    0.3486\n",
            "7  Notre Dame     0.0800    0.2848\n",
            "8         LSU     0.0759    0.2776\n",
            "9       Miami     0.0626    0.2042\n",
            "\n",
            "(Info) Teams included:\n",
            "         team     conf\n",
            "0       Texas      SEC\n",
            "1     Alabama      SEC\n",
            "2     Georgia      SEC\n",
            "3  Ohio State  Big Ten\n",
            "4  Penn State  Big Ten\n",
            "5     Clemson      ACC\n",
            "6      Oregon   Pac-12\n",
            "7  Notre Dame      Ind\n",
            "8         LSU      SEC\n",
            "9       Miami      ACC\n",
            "\n",
            "(Info) Features used & weights after availability check:\n",
            "{'FPI': 0.36363636363636365, 'TalentPoints': 0.2727272727272727, 'ReturningStartersPct': 0.18181818181818182, 'QB_Snaps': 0.18181818181818182}\n",
            "\n",
            "Saved: monte_carlo_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# NCAA Monte Carlo (3,000 trials) — full pipeline\n",
        "# =========================\n",
        "\n",
        "# --- Imports ---\n",
        "import os, glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from io import StringIO\n",
        "\n",
        "# -------------------------\n",
        "# 1) Load ALL tables from disk (.txt/.csv) with flexible parsing\n",
        "# -------------------------\n",
        "def read_any_table(path):\n",
        "    try:\n",
        "        return pd.read_csv(path, sep=None, engine=\"python\")\n",
        "    except Exception:\n",
        "        try:\n",
        "            return pd.read_csv(path, delim_whitespace=True, engine=\"python\")\n",
        "        except Exception:\n",
        "            try:\n",
        "                return pd.read_csv(path)\n",
        "            except Exception:\n",
        "                return None\n",
        "\n",
        "def load_all_local_tables(search_dirs=(\".\", \"./data\", \"/content\")):\n",
        "    paths = []\n",
        "    for d in search_dirs:\n",
        "        if os.path.isdir(d):\n",
        "            paths.extend(glob.glob(os.path.join(d, \"*.txt\")))\n",
        "            paths.extend(glob.glob(os.path.join(d, \"*.csv\")))\n",
        "    frames = []\n",
        "    for p in paths:\n",
        "        df = read_any_table(p)\n",
        "        if df is not None and len(df) > 0:\n",
        "            df[\"__source_file\"] = os.path.basename(p)\n",
        "            frames.append(df)\n",
        "    return pd.concat(frames, ignore_index=True, sort=False) if frames else pd.DataFrame()\n",
        "\n",
        "files_df = load_all_local_tables()\n",
        "\n",
        "# -------------------------\n",
        "# 2) Pull in arrays you added (if they exist) e.g., teams_data = [{...}, ...]\n",
        "# -------------------------\n",
        "arrays_df = pd.DataFrame()\n",
        "for var_name in [\"teams_data\", \"ratings_data\", \"coaches_data\", \"roster_data\"]:\n",
        "    try:\n",
        "        _val = globals().get(var_name, None)\n",
        "        if isinstance(_val, (list, tuple)) and len(_val) > 0:\n",
        "            arrays_df = pd.concat([arrays_df, pd.DataFrame(_val)], ignore_index=True, sort=False)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# -------------------------\n",
        "# 3) Combine & standardize columns\n",
        "# -------------------------\n",
        "raw = pd.concat([files_df, arrays_df], ignore_index=True, sort=False)\n",
        "\n",
        "rename_map_candidates = {\n",
        "    \"team\": [\"team\",\"Team\",\"school\",\"School\",\"program\",\"Program\"],\n",
        "    \"conf\": [\"conf\",\"conference\",\"Conference\",\"Conf\"],\n",
        "    \"FPI\": [\"FPI\",\"fpi\",\"espn_fpi\",\"espnFPI\"],\n",
        "    \"TalentPoints\": [\"TalentPoints\",\"talent_points\",\"Talent\",\"Talent_Points\",\"BlueChipPoints\",\"blue_chip_points\"],\n",
        "    \"ReturningStartersPct\": [\"ReturningStartersPct\",\"ReturningStarters%\",\"ReturningPct\",\"returning_pct\",\"Returning_Starters\"],\n",
        "    \"QB_Snaps\": [\"QB_Snaps\",\"QB Snaps\",\"qb_snaps\",\"qb_total_snaps\"],\n",
        "    \"coach_career_win_pct\": [\"coach_career_win_pct\",\"CoachWinPct\",\"coach_win_pct\",\"coachCareerWinPct\",\"Coach_Career_Win_Pct\"],\n",
        "    \"OVR\": [\"OVR\",\"Overall\",\"overall_rating\",\"overall\"],\n",
        "    \"OFF\": [\"OFF\",\"Offense\",\"off_rating\",\"OFF_Rating\"],\n",
        "    \"DEF\": [\"DEF\",\"Defense\",\"def_rating\",\"DEF_Rating\"],\n",
        "    \"QB_AVG\": [\"QB_AVG\",\"QB_Avg\",\"QB_Rating\",\"qb_overall\",\"QB\"],\n",
        "    \"Top3_OFF_AVG\": [\"Top3_OFF_AVG\",\"Top3_Off_Avg\",\"TopOff3\",\"Top3_Offense\"],\n",
        "    \"Top3_DEF_AVG\": [\"Top3_DEF_AVG\",\"Top3_Def_Avg\",\"TopDef3\",\"Top3_Defense\"],\n",
        "}\n",
        "\n",
        "def smart_rename_columns(df):\n",
        "    colmap = {}\n",
        "    lower_cols = {c.lower(): c for c in df.columns}\n",
        "    for canonical, options in rename_map_candidates.items():\n",
        "        for opt in options:\n",
        "            if opt in df.columns:\n",
        "                colmap[opt] = canonical\n",
        "                break\n",
        "            opt_low = opt.lower()\n",
        "            if opt_low in lower_cols:\n",
        "                colmap[lower_cols[opt_low]] = canonical\n",
        "                break\n",
        "    return df.rename(columns=colmap)\n",
        "\n",
        "if raw.empty:\n",
        "    raise ValueError(\"No data found. Put your .txt/.csv files in the working dir OR define arrays like `teams_data`.\")\n",
        "\n",
        "raw = smart_rename_columns(raw)\n",
        "\n",
        "wanted_cols = [\"team\",\"conf\",\"FPI\",\"TalentPoints\",\"ReturningStartersPct\",\"QB_Snaps\",\n",
        "               \"coach_career_win_pct\",\"OVR\",\"OFF\",\"DEF\",\"QB_AVG\",\"Top3_OFF_AVG\",\"Top3_DEF_AVG\",\"__source_file\"]\n",
        "present_cols = [c for c in wanted_cols if c in raw.columns]\n",
        "data = raw[present_cols].copy()\n",
        "\n",
        "if \"team\" not in data.columns:\n",
        "    raise ValueError(\"A 'team' column is required in your data.\")\n",
        "\n",
        "# Deduplicate by team (keep latest)\n",
        "data = data.dropna(subset=[\"team\"]).drop_duplicates(subset=[\"team\"], keep=\"last\")\n",
        "\n",
        "# Coerce numerics\n",
        "for num in [\"FPI\",\"TalentPoints\",\"ReturningStartersPct\",\"QB_Snaps\",\"coach_career_win_pct\",\n",
        "            \"OVR\",\"OFF\",\"DEF\",\"QB_AVG\",\"Top3_OFF_AVG\",\"Top3_DEF_AVG\"]:\n",
        "    if num in data.columns:\n",
        "        data[num] = pd.to_numeric(data[num], errors=\"coerce\")\n",
        "\n",
        "# -------------------------\n",
        "# 4) Ensure missing rating columns exist, then fill for ALL 10 teams\n",
        "# -------------------------\n",
        "for col in [\"coach_career_win_pct\",\"OVR\",\"OFF\",\"DEF\",\"QB_AVG\",\"Top3_OFF_AVG\",\"Top3_DEF_AVG\"]:\n",
        "    if col not in data.columns:\n",
        "        data[col] = np.nan\n",
        "\n",
        "# Ratings for your 10 teams (edit if your team names differ)\n",
        "ratings_updates = {\n",
        "    \"Texas\":       {\"coach_career_win_pct\": 75, \"OVR\": 95, \"OFF\": 94, \"DEF\": 93, \"QB_AVG\": 92, \"Top3_OFF_AVG\": 96, \"Top3_DEF_AVG\": 94},\n",
        "    \"Alabama\":     {\"coach_career_win_pct\": 80, \"OVR\": 94, \"OFF\": 93, \"DEF\": 94, \"QB_AVG\": 90, \"Top3_OFF_AVG\": 95, \"Top3_DEF_AVG\": 95},\n",
        "    \"Georgia\":     {\"coach_career_win_pct\": 78, \"OVR\": 94, \"OFF\": 92, \"DEF\": 95, \"QB_AVG\": 91, \"Top3_OFF_AVG\": 94, \"Top3_DEF_AVG\": 96},\n",
        "    \"Ohio State\":  {\"coach_career_win_pct\": 77, \"OVR\": 93, \"OFF\": 93, \"DEF\": 92, \"QB_AVG\": 90, \"Top3_OFF_AVG\": 95, \"Top3_DEF_AVG\": 93},\n",
        "    \"Penn State\":  {\"coach_career_win_pct\": 70, \"OVR\": 91, \"OFF\": 90, \"DEF\": 91, \"QB_AVG\": 88, \"Top3_OFF_AVG\": 92, \"Top3_DEF_AVG\": 91},\n",
        "    \"Clemson\":     {\"coach_career_win_pct\": 73, \"OVR\": 90, \"OFF\": 90, \"DEF\": 89, \"QB_AVG\": 87, \"Top3_OFF_AVG\": 91, \"Top3_DEF_AVG\": 90},\n",
        "    \"Oregon\":      {\"coach_career_win_pct\": 72, \"OVR\": 91, \"OFF\": 92, \"DEF\": 88, \"QB_AVG\": 89, \"Top3_OFF_AVG\": 93, \"Top3_DEF_AVG\": 90},\n",
        "    \"Notre Dame\":  {\"coach_career_win_pct\": 74, \"OVR\": 90, \"OFF\": 89, \"DEF\": 89, \"QB_AVG\": 88, \"Top3_OFF_AVG\": 90, \"Top3_DEF_AVG\": 91},\n",
        "    \"LSU\":         {\"coach_career_win_pct\": 71, \"OVR\": 91, \"OFF\": 91, \"DEF\": 90, \"QB_AVG\": 88, \"Top3_OFF_AVG\": 93, \"Top3_DEF_AVG\": 92},\n",
        "    \"Miami\":       {\"coach_career_win_pct\": 69, \"OVR\": 88, \"OFF\": 87, \"DEF\": 88, \"QB_AVG\": 86, \"Top3_OFF_AVG\": 89, \"Top3_DEF_AVG\": 88},\n",
        "}\n",
        "\n",
        "# Apply where team names match\n",
        "for team, vals in ratings_updates.items():\n",
        "    mask = data['team'].str.strip().str.casefold() == team.casefold()\n",
        "    for col, val in vals.items():\n",
        "        data.loc[mask, col] = data.loc[mask, col].fillna(val)\n",
        "\n",
        "# For any remaining NaNs in these columns, backfill with column medians (prevents dropping teams)\n",
        "for col in [\"coach_career_win_pct\",\"OVR\",\"OFF\",\"DEF\",\"QB_AVG\",\"Top3_OFF_AVG\",\"Top3_DEF_AVG\"]:\n",
        "    if col in data.columns:\n",
        "        if data[col].isna().any():\n",
        "            data[col] = data[col].fillna(data[col].median())\n",
        "\n",
        "# -------------------------\n",
        "# 5) Normalize features & compute strength\n",
        "# -------------------------\n",
        "feature_weights = {\n",
        "    # team quality + roster/talent\n",
        "    \"FPI\": 0.12,\n",
        "    \"TalentPoints\": 0.10,\n",
        "    \"ReturningStartersPct\": 0.08,\n",
        "    \"QB_Snaps\": 0.07,\n",
        "    # coaching + EA team ratings\n",
        "    \"coach_career_win_pct\": 0.10,\n",
        "    \"OVR\": 0.12,\n",
        "    \"OFF\": 0.08,\n",
        "    \"DEF\": 0.08,\n",
        "    \"QB_AVG\": 0.07,\n",
        "    # NEW: top-3 impact players\n",
        "    \"Top3_OFF_AVG\": 0.09,\n",
        "    \"Top3_DEF_AVG\": 0.09,\n",
        "}\n",
        "# keep only existing columns, then renormalize to sum=1\n",
        "feature_weights = {k:v for k,v in feature_weights.items() if k in data.columns}\n",
        "w_sum = sum(feature_weights.values())\n",
        "feature_weights = {k: v / w_sum for k,v in feature_weights.items()}\n",
        "\n",
        "def minmax(series):\n",
        "    s = pd.to_numeric(series, errors='coerce').astype(float)\n",
        "    lo, hi = s.min(), s.max()\n",
        "    if pd.isna(lo) or pd.isna(hi) or lo == hi:\n",
        "        return pd.Series([0.5]*len(s), index=s.index)\n",
        "    return (s - lo) / (hi - lo)\n",
        "\n",
        "norm = pd.DataFrame(index=data.index)\n",
        "for feat in feature_weights.keys():\n",
        "    norm[f\"norm_{feat}\"] = minmax(data[feat])\n",
        "\n",
        "def compute_strength(row):\n",
        "    return sum(feature_weights[feat] * row[f\"norm_{feat}\"] for feat in feature_weights.keys())\n",
        "\n",
        "np.random.seed(42)\n",
        "data[\"strength\"] = norm.apply(compute_strength, axis=1) + np.random.normal(0, 1e-6, size=len(data))\n",
        "\n",
        "# -------------------------\n",
        "# 6) Choose teams (top 10 by strength)\n",
        "# -------------------------\n",
        "USE_TOP_10 = True\n",
        "sim_df = data.sort_values(\"strength\", ascending=False).head(10).reset_index(drop=True) if USE_TOP_10 else data.reset_index(drop=True)\n",
        "\n",
        "# -------------------------\n",
        "# 7) Monte Carlo (3,000 trials), round-robin\n",
        "# -------------------------\n",
        "def monte_carlo_round_robin(df, trials=3000, seed=2025):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    teams = df[\"team\"].tolist()\n",
        "    strengths = df[\"strength\"].to_numpy()\n",
        "    n = len(teams)\n",
        "    wins = np.zeros(n, dtype=np.int64)\n",
        "\n",
        "    for _ in range(trials):\n",
        "        for i in range(n):\n",
        "            for j in range(i+1, n):\n",
        "                p_i = strengths[i] / (strengths[i] + strengths[j])\n",
        "                if rng.random() < p_i:\n",
        "                    wins[i] += 1\n",
        "                else:\n",
        "                    wins[j] += 1\n",
        "\n",
        "    total_games = (n * (n - 1) // 2) * trials\n",
        "    win_share = wins / total_games\n",
        "    results = pd.DataFrame({\"Team\": teams, \"Win_Share\": win_share, \"Strength\": strengths})\n",
        "    return results.sort_values(\"Win_Share\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "# -------------------------\n",
        "# 8) Run & print\n",
        "# -------------------------\n",
        "results = monte_carlo_round_robin(sim_df, trials=3000, seed=2025)\n",
        "\n",
        "pd.set_option(\"display.precision\", 4)\n",
        "print(\"\\n=== Monte Carlo Results (3,000 trials, all variables incl. Top3 players) ===\")\n",
        "print(results)\n",
        "\n",
        "print(\"\\n(Info) Teams included:\")\n",
        "cols = [\"team\",\"conf\"] if \"conf\" in sim_df.columns else [\"team\"]\n",
        "print(sim_df[cols])\n",
        "\n",
        "print(\"\\n(Info) Features used & final weights:\")\n",
        "print(feature_weights)\n",
        "\n",
        "# Optional: save\n",
        "results.to_csv(\"monte_carlo_results_all_vars.csv\", index=False)\n",
        "print(\"\\nSaved: monte_carlo_results_all_vars.csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "5WwvWY95AGP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- Monte Carlo WITHOUT round-robin ---\n",
        "# Each season: every team plays G games against randomly chosen opponents\n",
        "# Home-field adds a small bump to the home team.\n",
        "def monte_carlo_random_schedule(df, trials=3000, games_per_team=12, home_field_adv=0.03, seed=2025):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    teams = df[\"team\"].tolist()\n",
        "    strengths = df[\"strength\"].to_numpy()\n",
        "    n = len(teams)\n",
        "\n",
        "    # trackers across all trials\n",
        "    avg_winrate = np.zeros(n, dtype=float)\n",
        "\n",
        "    for _ in range(trials):\n",
        "        # per-season counters\n",
        "        wins = np.zeros(n, dtype=int)\n",
        "        remaining = np.full(n, games_per_team, dtype=int)\n",
        "\n",
        "        # schedule games until every team has played `games_per_team`\n",
        "        while remaining.sum() > 0:\n",
        "            # pick team A that still needs a game\n",
        "            i_candidates = np.where(remaining > 0)[0]\n",
        "            i = rng.choice(i_candidates)\n",
        "\n",
        "            # pick opponent B that still needs a game (not i)\n",
        "            j_candidates = i_candidates[i_candidates != i]\n",
        "            if j_candidates.size == 0:\n",
        "                # if no opponent available due to odd scheduling (rare), skip\n",
        "                remaining[i] = 0\n",
        "                continue\n",
        "            j = rng.choice(j_candidates)\n",
        "\n",
        "            # random home team\n",
        "            home_is_i = rng.random() < 0.5\n",
        "\n",
        "            # base win prob using Bradley–Terry style\n",
        "            p_i = strengths[i] / (strengths[i] + strengths[j])\n",
        "\n",
        "            # apply home field advantage\n",
        "            if home_is_i:\n",
        "                p_i = np.clip(p_i + home_field_adv, 0.01, 0.99)\n",
        "            else:\n",
        "                p_i = np.clip(p_i - home_field_adv, 0.01, 0.99)\n",
        "\n",
        "            # play game\n",
        "            if rng.random() < p_i:\n",
        "                wins[i] += 1\n",
        "            else:\n",
        "                wins[j] += 1\n",
        "\n",
        "            # both teams used one game slot\n",
        "            remaining[i] -= 1\n",
        "            remaining[j] -= 1\n",
        "\n",
        "        # season win rate for each team\n",
        "        winrate = wins / games_per_team\n",
        "        avg_winrate += winrate\n",
        "\n",
        "    # average across trials\n",
        "    avg_winrate /= trials\n",
        "\n",
        "    results = pd.DataFrame({\n",
        "        \"Team\": teams,\n",
        "        \"Avg_WinRate\": avg_winrate,\n",
        "        \"Strength\": strengths\n",
        "    }).sort_values(\"Avg_WinRate\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "    return results\n",
        "\n",
        "# ---- Run it (no round-robin) ----\n",
        "results_no_rr = monte_carlo_random_schedule(data, trials=3000, games_per_team=12, home_field_adv=0.03, seed=2025)\n",
        "print(\"\\n=== Monte Carlo Results (3,000 seasons, RANDOM schedules, no round-robin) ===\")\n",
        "print(results_no_rr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5AtfH2SoC7Mc",
        "outputId": "607b3ed8-7437-4a76-f6cc-99d105eddeed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Monte Carlo Results (3,000 seasons, RANDOM schedules, no round-robin) ===\n",
            "         Team  Avg_WinRate  Strength\n",
            "0       Texas       0.6581    0.8276\n",
            "1     Alabama       0.6482    0.7939\n",
            "2     Georgia       0.6292    0.7427\n",
            "3  Ohio State       0.5973    0.6365\n",
            "4  Penn State       0.4839    0.4129\n",
            "5      Oregon       0.4644    0.3811\n",
            "6         LSU       0.4546    0.3687\n",
            "7     Clemson       0.4479    0.3480\n",
            "8  Notre Dame       0.4060    0.2975\n",
            "9       Miami       0.1465    0.0830\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "def plot_rankings(df, metric_col, title):\n",
        "    df_plot = df.copy().sort_values(metric_col, ascending=True)\n",
        "    plt.figure(figsize=(9, 6))\n",
        "    plt.barh(df_plot[\"Team\"], df_plot[metric_col])\n",
        "    plt.xlabel(metric_col)\n",
        "    plt.title(title)\n",
        "    for i, v in enumerate(df_plot[metric_col]):\n",
        "        plt.text(v, i, f\" {v:.3f}\", va=\"center\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Choose the results you produced:\n",
        "if \"results_no_rr\" in globals():\n",
        "    plot_rankings(results_no_rr, \"Avg_WinRate\", \"Average Win Rate (3,000 seasons, random schedules)\")\n",
        "elif \"results\" in globals():\n",
        "    plot_rankings(results, \"Win_Share\", \"Win Share (3,000 trials, round-robin)\")\n"
      ],
      "metadata": {
        "id": "kRb20CX2GeB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "week1_results_model = [\n",
        "    {\"winner\": \"Ohio State\", \"winner_rank\": 3, \"loser\": \"Texas\", \"loser_rank\": 1, \"score\": \"14-7\"},\n",
        "    {\"winner\": \"Penn State\", \"winner_rank\": 2, \"loser\": \"Nevada\", \"loser_rank\": None, \"score\": \"46-11\"},\n",
        "    {\"winner\": \"LSU\", \"winner_rank\": 9, \"loser\": \"Clemson\", \"loser_rank\": 4, \"score\": \"17-10\"},\n",
        "    {\"winner\": \"Georgia\", \"winner_rank\": 5, \"loser\": \"Marshall\", \"loser_rank\": None, \"score\": \"45-7\"},\n",
        "    {\"winner\": \"Oregon\", \"winner_rank\": 7, \"loser\": \"Montana State\", \"loser_rank\": None, \"score\": \"59-13\"},\n",
        "    {\"winner\": \"Alabama\", \"winner_rank\": 8, \"loser\": \"Florida State\", \"loser_rank\": None, \"score\": \"31-17\"},\n",
        "    # Miami and Notre Dame didn't have games listed here\n",
        "]\n"
      ],
      "metadata": {
        "id": "pazis8i9rSr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== WEEK 1 RESULTS -> RATING UPDATE -> RE-RUN TRIALS =====================\n",
        "\n",
        "# --- 0) Week 1 results: only teams in your 10-team model ---\n",
        "week1_results_model = [\n",
        "    {\"winner\": \"Ohio State\", \"winner_rank\": 3, \"loser\": \"Texas\", \"loser_rank\": 1, \"score\": \"14-7\"},\n",
        "    {\"winner\": \"Penn State\", \"winner_rank\": 2, \"loser\": \"Nevada\", \"loser_rank\": None, \"score\": \"46-11\"},\n",
        "    {\"winner\": \"LSU\", \"winner_rank\": 9, \"loser\": \"Clemson\", \"loser_rank\": 4, \"score\": \"17-10\"},\n",
        "    {\"winner\": \"Georgia\", \"winner_rank\": 5, \"loser\": \"Marshall\", \"loser_rank\": None, \"score\": \"45-7\"},\n",
        "    {\"winner\": \"Oregon\", \"winner_rank\": 7, \"loser\": \"Montana State\", \"loser_rank\": None, \"score\": \"59-13\"},\n",
        "    {\"winner\": \"Alabama\", \"winner_rank\": 8, \"loser\": \"Florida State\", \"loser_rank\": None, \"score\": \"31-17\"},\n",
        "    # Miami and Notre Dame not listed in your Week 1 results\n",
        "]\n",
        "\n",
        "# --- 1) Helper: get rating values safely ---\n",
        "def _get_one(series):\n",
        "    \"\"\"Return a single scalar from a 1-row series (.item() is cleaner than float(series)).\"\"\"\n",
        "    return series.item()\n",
        "\n",
        "# --- 2) Win probability (same shape as earlier, with a softer scale) ---\n",
        "SCALE = 6.0\n",
        "def win_prob(r_i, r_j, scale=SCALE):\n",
        "    return 1.0 / (1.0 + np.exp(-(r_i - r_j) / scale))\n",
        "\n",
        "# --- 3) Apply Elo-style post-game updates (gentle; includes margin-of-victory damping) ---\n",
        "def apply_results_update(df_in, results, K=0.9, cap_mov=28):\n",
        "    \"\"\"\n",
        "    df_in: DataFrame with columns ['team','rating'] (your fair rating)\n",
        "    results: list of dicts with keys winner, loser, score (\"A-B\")\n",
        "    K: learning rate (0.6–1.2 reasonable). Lower => smaller moves.\n",
        "    cap_mov: cap on absolute point differential for stability.\n",
        "    \"\"\"\n",
        "    df_upd = df_in.copy()\n",
        "    df_upd = df_upd.set_index(\"team\")\n",
        "\n",
        "    for g in results:\n",
        "        w, l = g[\"winner\"], g[\"loser\"]\n",
        "        # skip games where the opponent is not in the model (e.g., Nevada, Marshall)\n",
        "        if w not in df_upd.index or l not in df_upd.index:\n",
        "            continue\n",
        "\n",
        "        # parse margin of victory\n",
        "        try:\n",
        "            a, b = g[\"score\"].split(\"-\")\n",
        "            mov = max(0, min(cap_mov, abs(int(a) - int(b))))\n",
        "        except Exception:\n",
        "            mov = 0  # if score not parseable, ignore MOV\n",
        "\n",
        "        # ratings before update\n",
        "        rw = df_upd.at[w, \"rating\"]\n",
        "        rl = df_upd.at[l, \"rating\"]\n",
        "\n",
        "        # expected win prob for winner vs loser\n",
        "        p_exp = win_prob(rw, rl, scale=SCALE)\n",
        "\n",
        "        # margin multiplier: log(1+mov) keeps big blowouts from exploding updates\n",
        "        mov_mult = np.log1p(mov) / np.log(8)  # ~1.0 around 7–8 pts, <1 for small wins, >1 for big wins\n",
        "        mov_mult = float(np.clip(mov_mult, 0.5, 1.5))\n",
        "\n",
        "        # update (winner moves up by (1 - p), loser down by same amount)\n",
        "        delta = K * mov_mult * (1.0 - p_exp)\n",
        "        df_upd.at[w, \"rating\"] = rw + delta\n",
        "        df_upd.at[l, \"rating\"] = rl - delta\n",
        "\n",
        "    # re-standardize so the field mean=0, std=1 (keeps compatibility with your sim scale)\n",
        "    mu = df_upd[\"rating\"].mean()\n",
        "    sd = df_upd[\"rating\"].std(ddof=0) + 1e-9\n",
        "    df_upd[\"rating\"] = (df_upd[\"rating\"] - mu) / sd\n",
        "\n",
        "    return df_upd.reset_index()\n",
        "\n",
        "df_updated = apply_results_update(df[[\"team\", \"rating\"]], week1_results_model, K=0.9, cap_mov=28)\n",
        "print(\"Ratings updated with Week 1 results (top 10):\")\n",
        "print(df_updated.sort_values(\"rating\", ascending=False).head(10))\n",
        "\n",
        "# --- 4) Minimal simulation utilities (use these if you don't already have them defined) ---\n",
        "def simulate_season(n_trials, table, scale=SCALE, seed=None):\n",
        "    \"\"\"\n",
        "    Round-robin neutral-site season repeated n_trials times.\n",
        "    Returns average win share per team.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    teams = table[\"team\"].tolist()\n",
        "    rmap = dict(zip(table[\"team\"], table[\"rating\"]))\n",
        "\n",
        "    wins = dict((t, 0.0) for t in teams)\n",
        "    games_per_team = len(teams) - 1\n",
        "\n",
        "    for _ in range(n_trials):\n",
        "        for i in range(len(teams)):\n",
        "            for j in range(i + 1, len(teams)):\n",
        "                ti, tj = teams[i], teams[j]\n",
        "                p = win_prob(rmap[ti], rmap[tj], scale=scale)\n",
        "                if rng.random() < p:\n",
        "                    wins[ti] += 1\n",
        "                else:\n",
        "                    wins[tj] += 1\n",
        "\n",
        "    # convert to average share\n",
        "    share = {t: wins[t] / (n_trials * games_per_team) for t in teams}\n",
        "    return pd.Series(share).sort_values(ascending=False)\n",
        "\n",
        "def run_trials(table, n_trials=2400, seed=2025):\n",
        "    \"\"\"Return summary table with average win share and simple 'champ' frequency (most wins each trial).\"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    teams = table[\"team\"].tolist()\n",
        "    rmap = dict(zip(table[\"team\"], table[\"rating\"]))\n",
        "\n",
        "    champ_counts = dict((t, 0) for t in teams)\n",
        "    games_per_team = len(teams) - 1\n",
        "    wins_accum = dict((t, 0.0) for t in teams)\n",
        "\n",
        "    for t in range(n_trials):\n",
        "        # simulate one full round-robin season\n",
        "        wins = dict((tname, 0) for tname in teams)\n",
        "        for i in range(len(teams)):\n",
        "            for j in range(i + 1, len(teams)):\n",
        "                ti, tj = teams[i], teams[j]\n",
        "                p = win_prob(rmap[ti], rmap[tj], scale=SCALE)\n",
        "                if rng.random() < p:\n",
        "                    wins[ti] += 1\n",
        "                else:\n",
        "                    wins[tj] += 1\n",
        "        # accumulate\n",
        "        for k in teams:\n",
        "            wins_accum[k] += wins[k]\n",
        "        # champion = max wins (break ties randomly)\n",
        "        max_w = max(wins.values())\n",
        "        tied = [k for k, v in wins.items() if v == max_w]\n",
        "        champ = rng.choice(tied)\n",
        "        champ_counts[champ] += 1\n",
        "\n",
        "    avg_share = {k: wins_accum[k] / (n_trials * games_per_team) for k in teams}\n",
        "    champ_prob = {k: champ_counts[k] / n_trials for k in teams}\n",
        "\n",
        "    out = pd.DataFrame({\n",
        "        \"team\": teams,\n",
        "        \"avg_win_share\": [avg_share[k] for k in teams],\n",
        "        \"champ_prob\": [champ_prob[k] for k in teams],\n",
        "        \"rating\": [rmap[k] for k in teams],\n",
        "    }).sort_values([\"champ_prob\", \"avg_win_share\"], ascending=False, ignore_index=True)\n",
        "\n",
        "    return out\n",
        "\n",
        "# --- 5) RUN: Monte-Carlo on updated ratings ---\n",
        "results_after_w1 = run_trials(df_updated, n_trials=2400, seed=2026)\n",
        "print(\"\\nMonte Carlo (after Week 1 updates) — top 10:\")\n",
        "print(results_after_w1.head(10))\n"
      ],
      "metadata": {
        "id": "Fa8KqfBmrp42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you already have df_updated or a merged table with rating, rename it back to df\n",
        "# Replace df_updated with your actual DataFrame that contains \"team\" and \"rating\"\n",
        "df = df_updated  # <-- Or assign your merged DataFrame here\n"
      ],
      "metadata": {
        "id": "UjPNaE78r1y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_updated = apply_results_update(df[[\"team\", \"rating\"]], week1_results_model, K=0.9, cap_mov=28)\n",
        "print(\"Ratings updated with Week 1 results (top 10):\")\n",
        "print(df_updated.sort_values(\"rating\", ascending=False).head(10))\n"
      ],
      "metadata": {
        "id": "6cmd2N8Cr5LQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================= FULL NCAA MODEL RESET & WEEK 1 SIM =================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1) BASE DATA: from your earlier array\n",
        "teams_data = [\n",
        "    {\"team\": \"Texas\", \"conf\": \"SEC\", \"FPI\": 24.5, \"TalentRank\": 4,\n",
        "     \"TalentPoints\": 973.54, \"ReturningStartersPct\": 55.0, \"QB_Snaps\": 260},\n",
        "    {\"team\": \"Alabama\", \"conf\": \"SEC\", \"FPI\": 20.7, \"TalentRank\": 2,\n",
        "     \"TalentPoints\": 993.55, \"ReturningStartersPct\": 63.6, \"QB_Snaps\": 60},\n",
        "    {\"team\": \"Ohio State\", \"conf\": \"Big Ten\", \"FPI\": 20.5, \"TalentRank\": 3,\n",
        "     \"TalentPoints\": 973.69, \"ReturningStartersPct\": 50.0, \"QB_Snaps\": 180},\n",
        "    {\"team\": \"Georgia\", \"conf\": \"SEC\", \"FPI\": 19.8, \"TalentRank\": 1,\n",
        "     \"TalentPoints\": 999.9, \"ReturningStartersPct\": 58.0, \"QB_Snaps\": 40},\n",
        "    {\"team\": \"Penn State\", \"conf\": \"Big Ten\", \"FPI\": 18.2, \"TalentRank\": 6,\n",
        "     \"TalentPoints\": 950.1, \"ReturningStartersPct\": 62.0, \"QB_Snaps\": 150},\n",
        "    {\"team\": \"Clemson\", \"conf\": \"ACC\", \"FPI\": 17.0, \"TalentRank\": 7,\n",
        "     \"TalentPoints\": 940.0, \"ReturningStartersPct\": 60.0, \"QB_Snaps\": 220},\n",
        "    {\"team\": \"Notre Dame\", \"conf\": \"Ind\", \"FPI\": 16.5, \"TalentRank\": 8,\n",
        "     \"TalentPoints\": 930.0, \"ReturningStartersPct\": 61.0, \"QB_Snaps\": 75},\n",
        "    {\"team\": \"Oregon\", \"conf\": \"Pac-12\", \"FPI\": 15.8, \"TalentRank\": 5,\n",
        "     \"TalentPoints\": 960.0, \"ReturningStartersPct\": 59.0, \"QB_Snaps\": 100},\n",
        "    {\"team\": \"LSU\", \"conf\": \"SEC\", \"FPI\": 15.2, \"TalentRank\": 9,\n",
        "     \"TalentPoints\": 925.0, \"ReturningStartersPct\": 57.0, \"QB_Snaps\": 210},\n",
        "    {\"team\": \"Miami\", \"conf\": \"ACC\", \"FPI\": 14.9, \"TalentRank\": 10,\n",
        "     \"TalentPoints\": 915.0, \"ReturningStartersPct\": 56.0, \"QB_Snaps\": 190},\n",
        "]\n",
        "df = pd.DataFrame(teams_data)\n",
        "\n",
        "# 2) FAIR RATING CALC\n",
        "def robust_z(s: pd.Series):\n",
        "    med = s.median()\n",
        "    iqr = s.quantile(0.75) - s.quantile(0.25)\n",
        "    if iqr == 0:\n",
        "        std = s.std(ddof=0) + 1e-9\n",
        "        return (s - med) / std\n",
        "    return (s - med) / (iqr + 1e-9)\n",
        "\n",
        "numeric_cols = [\"FPI\",\"TalentPoints\",\"ReturningStartersPct\",\"QB_Snaps\"]\n",
        "for c in numeric_cols:\n",
        "    df[c+\"_rz\"] = robust_z(df[c]).clip(-1.5, 1.5)\n",
        "\n",
        "weights = {\n",
        "    \"FPI_rz\": 0.25, \"TalentPoints_rz\": 0.20,\n",
        "    \"ReturningStartersPct_rz\": 0.15, \"QB_Snaps_rz\": 0.10\n",
        "}\n",
        "total_w = sum(weights.values())\n",
        "weights = {k: v/total_w for k,v in weights.items()}\n",
        "\n",
        "df[\"feature_score\"] = sum(df[k]*w for k,w in weights.items())\n",
        "df[\"rating_fair_raw\"] = 0.7 * df[\"feature_score\"]\n",
        "mu, sd = df[\"rating_fair_raw\"].mean(), df[\"rating_fair_raw\"].std(ddof=0)+1e-9\n",
        "df[\"rating\"] = (df[\"rating_fair_raw\"] - mu)/sd\n",
        "\n",
        "# 3) WEEK 1 RESULTS\n",
        "week1_results_model = [\n",
        "    {\"winner\": \"Ohio State\", \"loser\": \"Texas\", \"score\": \"14-7\"},\n",
        "    {\"winner\": \"Penn State\", \"loser\": \"Nevada\", \"score\": \"46-11\"},\n",
        "    {\"winner\": \"LSU\", \"loser\": \"Clemson\", \"score\": \"17-10\"},\n",
        "    {\"winner\": \"Georgia\", \"loser\": \"Marshall\", \"score\": \"45-7\"},\n",
        "    {\"winner\": \"Oregon\", \"loser\": \"Montana State\", \"score\": \"59-13\"},\n",
        "    {\"winner\": \"Alabama\", \"loser\": \"Florida State\", \"score\": \"31-17\"},\n",
        "]\n",
        "\n",
        "SCALE = 6.0\n",
        "def win_prob(r1, r2, scale=SCALE):\n",
        "    return 1 / (1 + np.exp(-(r1 - r2)/scale))\n",
        "\n",
        "def apply_results_update(df_in, results, K=0.9):\n",
        "    df_upd = df_in.set_index(\"team\").copy()\n",
        "    for g in results:\n",
        "        w, l = g[\"winner\"], g[\"loser\"]\n",
        "        if w not in df_upd.index or l not in df_upd.index:  # skip non-model teams\n",
        "            continue\n",
        "        rw, rl = df_upd.at[w, \"rating\"], df_upd.at[l, \"rating\"]\n",
        "        p_exp = win_prob(rw, rl)\n",
        "        delta = K * (1 - p_exp)\n",
        "        df_upd.at[w, \"rating\"] = rw + delta\n",
        "        df_upd.at[l, \"rating\"] = rl - delta\n",
        "    # normalize ratings\n",
        "    mu, sd = df_upd[\"rating\"].mean(), df_upd[\"rating\"].std(ddof=0)+1e-9\n",
        "    df_upd[\"rating\"] = (df_upd[\"rating\"] - mu)/sd\n",
        "    return df_upd.reset_index()\n",
        "\n",
        "df_updated = apply_results_update(df, week1_results_model)\n",
        "\n",
        "# 4) SIMULATION\n",
        "def run_trials(table, n_trials=1000):\n",
        "    teams = table[\"team\"].tolist()\n",
        "    rmap = dict(zip(table[\"team\"], table[\"rating\"]))\n",
        "    wins = {t: 0 for t in teams}\n",
        "    games_per_team = len(teams)-1\n",
        "    rng = np.random.default_rng()\n",
        "    for _ in range(n_trials):\n",
        "        for i in range(len(teams)):\n",
        "            for j in range(i+1,len(teams)):\n",
        "                ti, tj = teams[i], teams[j]\n",
        "                p = win_prob(rmap[ti], rmap[tj])\n",
        "                if rng.random() < p:\n",
        "                    wins[ti]+=1\n",
        "                else:\n",
        "                    wins[tj]+=1\n",
        "    share = {t: wins[t]/(n_trials*games_per_team) for t in teams}\n",
        "    return pd.DataFrame({\"team\":teams,\"avg_win_share\":[share[t] for t in teams]}).sort_values(\"avg_win_share\",ascending=False)\n",
        "\n",
        "# 5) OUTPUT\n",
        "print(\"Updated ratings after Week 1:\")\n",
        "print(df_updated.sort_values(\"rating\",ascending=False))\n",
        "\n",
        "print(\"\\nSimulation results (avg win share):\")\n",
        "print(run_trials(df_updated, n_trials=2000))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZZmkG18r8uU",
        "outputId": "ca5a7a1b-0c17-45e7-c51d-28e104697bca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated ratings after Week 1:\n",
            "         team     conf   FPI  TalentRank  TalentPoints  ReturningStartersPct  \\\n",
            "1     Alabama      SEC  20.7           2        993.55                  63.6   \n",
            "0       Texas      SEC  24.5           4        973.54                  55.0   \n",
            "3     Georgia      SEC  19.8           1        999.90                  58.0   \n",
            "2  Ohio State  Big Ten  20.5           3        973.69                  50.0   \n",
            "4  Penn State  Big Ten  18.2           6        950.10                  62.0   \n",
            "7      Oregon   Pac-12  15.8           5        960.00                  59.0   \n",
            "5     Clemson      ACC  17.0           7        940.00                  60.0   \n",
            "8         LSU      SEC  15.2           9        925.00                  57.0   \n",
            "6  Notre Dame      Ind  16.5           8        930.00                  61.0   \n",
            "9       Miami      ACC  14.9          10        915.00                  56.0   \n",
            "\n",
            "   QB_Snaps    FPI_rz  TalentPoints_rz  ReturningStartersPct_rz  QB_Snaps_rz  \\\n",
            "1        60  0.712644         0.935545                 1.133333    -0.848485   \n",
            "0       260  1.500000         0.449304                -0.777778     0.767677   \n",
            "3        40  0.505747         1.089849                -0.111111    -1.010101   \n",
            "2       180  0.666667         0.452949                -1.500000     0.121212   \n",
            "4       150  0.137931        -0.120284                 0.777778    -0.121212   \n",
            "7       100 -0.413793         0.120284                 0.111111    -0.525253   \n",
            "5       220 -0.137931        -0.365713                 0.333333     0.444444   \n",
            "8       210 -0.551724        -0.730211                -0.333333     0.363636   \n",
            "6        75 -0.252874        -0.608711                 0.555556    -0.727273   \n",
            "9       190 -0.620690        -0.973209                -0.555556     0.202020   \n",
            "\n",
            "   feature_score  rating_fair_raw    rating  \n",
            "1       0.643459         0.450421  1.682661  \n",
            "0       0.607088         0.424962  1.039630  \n",
            "3       0.323900         0.226730  0.798288  \n",
            "2       0.063397         0.044378  0.619729  \n",
            "4       0.164245         0.114971  0.356448  \n",
            "7      -0.164643        -0.115250 -0.553741  \n",
            "5      -0.018830        -0.013181 -0.678474  \n",
            "8      -0.425157        -0.297610 -0.746439  \n",
            "6      -0.249078        -0.174355 -0.787413  \n",
            "9      -0.589922        -0.412946 -1.730691  \n",
            "\n",
            "Simulation results (avg win share):\n",
            "         team  avg_win_share\n",
            "1     Alabama       0.572056\n",
            "0       Texas       0.549167\n",
            "3     Georgia       0.528889\n",
            "2  Ohio State       0.526056\n",
            "4  Penn State       0.518000\n",
            "7      Oregon       0.475722\n",
            "8         LSU       0.474833\n",
            "5     Clemson       0.469167\n",
            "6  Notre Dame       0.461667\n",
            "9       Miami       0.424444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Lj5H1tFsiGz",
        "outputId": "59d95718-de00-4edd-da93-8398177e8ca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏈 Rankings with QB Snaps heavily weighted:\n",
            "         team  rating_qb\n",
            "0       Texas   2.338406\n",
            "5     Clemson   0.736919\n",
            "2  Ohio State   0.496955\n",
            "4  Penn State   0.195302\n",
            "8         LSU  -0.033723\n",
            "1     Alabama  -0.087250\n",
            "9       Miami  -0.534021\n",
            "3     Georgia  -0.805251\n",
            "7      Oregon  -0.930683\n",
            "6  Notre Dame  -1.376652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "so who is"
      ],
      "metadata": {
        "id": "NWhnNN2ys65N"
      }
    }
  ]
}